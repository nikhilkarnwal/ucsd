{"cells":[{"metadata":{},"cell_type":"markdown","source":"# SEED RL agent to play GFootball\nIn this notebook we present a way to train V-trace off-policy actor-critic agent introduced in [IMPALA: Scalable Distributed Deep-RL with Importance Weighted Actor-Learner Architectures](https://arxiv.org/pdf/1802.01561.pdf) using [SEED RL](https://github.com/google-research/seed_rl/) framework.\nAgent trained in this notebook can serve as a starting point, but as the main goal is to provide an interactive introduction into applying Deep-RL to GFootball, obtained agent is far from comprehensive. See the [Google Research Football: A Novel Reinforcement Learning Environment](https://arxiv.org/abs/1907.11180) paper for details on aplying V-trace to GFootball.\n\nThe first step is to install required tools:"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# Install:\n# GFootball environment (https://github.com/google-research/football/),\n# SEED RL for training an agent (https://github.com/google-research/seed_rl/),\n# Tensorflow 2.2, which is needed by SEED RL.\n\n!apt-get update\n!apt-get install -y libsdl2-gfx-dev libsdl2-ttf-dev\n!pip3 install tensorflow==2.2\n!pip3 install tensorflow_probability==0.9.0\n\n# Update kaggle-environments to the newest version.\n# !pip3 install kaggle-environments -U\n\n# # Make sure that the Branch in git clone and in wget call matches !!\n# !git clone -b v2.8 https://github.com/google-research/football.git\n# !mkdir -p football/third_party/gfootball_engine/lib\n\n# !wget https://storage.googleapis.com/gfootball/prebuilt_gameplayfootball_v2.8.so -O football/third_party/gfootball_engine/lib/prebuilt_gameplayfootball.so\n# !cd football && GFOOTBALL_USE_PREBUILT_SO=1 pip3 install .\n\n# !git clone https://github.com/google-research/seed_rl.git\n# !cd seed_rl && git checkout 34fb2874d41241eb4d5a03344619fb4e34dd9be6\n# !mkdir /kaggle_simulations/agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!mkdir /kaggle/working/kaggle_simulations/\n!mkdir /kaggle/working/kaggle_simulations/agent","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -r /kaggle/working/seed_rl/*.py","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"SEED RL provides scripts for running training on [local machine inside Docker](https://github.com/google-research/seed_rl/#local-machine-training-on-a-single-level) and [distributed training](https://github.com/google-research/seed_rl/#distributed-training-using-ai-platform) at scale using [AI Platform](https://cloud.google.com/ai-platform). To make it run inside a notebook we need to create a launcher script (based on [SEED's docker launcher script](https://github.com/google-research/seed_rl/blob/master/docker/run.sh)):"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile train.sh\n# Training launcher script.\n\n# Make SEED RL visible to Python.\nexport PYTHONPATH=$PYTHONPATH:$(pwd)\nENVIRONMENT=$1\nAGENT=$2\nNUM_ACTORS=$3\nshift 3\necho ${ENVIRONMENT}\n# Start actor tasks which run environment loop.\nactor=0\nwhile [ \"$actor\" -lt ${NUM_ACTORS} ]; do\n  python3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=actor --logtostderr $@ --num_actors=${NUM_ACTORS} --task=${actor} 2>/dev/null >/dev/null &\n  actor=$(( actor + 1 ))\ndone\n# Start learner task which performs training of the agent.\npython3 seed_rl/${ENVIRONMENT}/${AGENT}_main.py --run_mode=learner --logtostderr $@ --num_actors=\"${NUM_ACTORS}\"\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can run the training for the Kaggle competition scenario (11_vs_11_kaggle). As this example is meant to be interactive, we train for  10000 steps, which doesn't provide a good quality agent, but training should take only a few minutes."},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"!bash train.sh football vtrace 4 '--total_environment_frames=10000 --game=11_vs_11_kaggle --reward_experiment=scoring,checkpoints --logdir=/kaggle/working/kaggle_simulations/agent/'","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end of the training Tensorflow model is saved for later use."},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -la /kaggle_simulations/agent/saved_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets first try to visualize a game played by our trained agent. For that we need to implement a wrapper which loads Tensorflow model and converts observations provided by Kaggle environment to observations accepted by the SEED agent:"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%writefile /kaggle_simulations/agent/main.py\n\nimport collections\nimport gym\nimport numpy as np\nimport os\nimport sys\nimport tensorflow as tf\n\nfrom gfootball.env import observation_preprocessing\nfrom gfootball.env import wrappers\n\nEnvOutput = collections.namedtuple(\n    'EnvOutput', 'reward done observation abandoned episode_step')\n\ndef prepare_agent_input(observation, prev_action, state):\n    # SEED RL agent accepts input in a form of EnvOutput. When not training\n    # only observation is used for generating action, so we use a dummy values\n    # for the rest.\n    env_output = EnvOutput(reward=tf.zeros(shape=[], dtype=tf.float32),\n        done=tf.zeros(shape=[], dtype=tf.bool),\n        observation=observation, abandoned=False,\n        episode_step=tf.zeros(shape=[], dtype=tf.int32))\n    # add batch dimension\n    prev_action, env_output = tf.nest.map_structure(\n        lambda t: tf.expand_dims(t, 0), (prev_action, env_output))\n\n    return (prev_action, env_output, state)\n\n# Previously executed action\nprevious_action = tf.constant(0, dtype=tf.int64)\n# Queue of recent observations (SEED agent we trained uses frame stacking).\nobservations = collections.deque([], maxlen=4)\n# Current state of the agent (used by recurrent agents).\nstate = ()\n\n# Load previously trained Tensorflow model.\npolicy = tf.compat.v2.saved_model.load('/kaggle_simulations/agent/saved_model')\nstep_nr = 0\n\ndef agent(obs):\n    global step_nr\n    global previous_action\n    global observations\n    global state\n    global policy\n    # Get observations for the first (and only one) player we control.\n    obs = obs['players_raw'][0]\n    # Agent we trained uses Super Mini Map (SMM) representation.\n    # See https://github.com/google-research/seed_rl/blob/master/football/env.py for details.\n    obs = observation_preprocessing.generate_smm([obs])[0]\n    print(obs.shape)\n    if not observations:\n        observations.extend([obs] * 4)\n    else:\n        observations.append(obs)\n    \n    # SEED packs observations to reduce transfer times.\n    # See PackedBitsObservation in https://github.com/google-research/seed_rl/blob/master/football/observation.py\n    obs = np.concatenate(list(observations), axis=-1)\n    obs = np.packbits(obs, axis=-1)\n    if obs.shape[-1] % 2 == 1:\n        obs = np.pad(obs, [(0, 0)] * (obs.ndim - 1) + [(0, 1)], 'constant')\n    obs = obs.view(np.uint16)\n\n    # Execute our agent to obtain action to take.\n    enc = lambda x: x\n    dec = lambda x, s=None: x if s is None else tf.nest.pack_sequence_as(s, x)\n    agent_output, state = policy.get_action(*dec(enc(prepare_agent_input(obs, previous_action, state))))\n    previous_action = agent_output.action[0]\n    return [int(previous_action)]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can easily visualize behavior of our agent in action:"},{"metadata":{"trusted":true},"cell_type":"code","source":"from kaggle_environments import make\nenv = make(\"football\", configuration={\"save_video\": True, \"scenario_name\": \"11_vs_11_kaggle\", \"running_in_notebook\": True})\nenv.run([\"/kaggle_simulations/agent/main.py\", \"run_right\"])\nenv.render(mode=\"human\", width=800, height=600)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Prepare a submision package containing trained model and the main execution logic.\n!cd /kaggle_simulations/agent && tar -czvf /kaggle/working/submit.tar.gz main.py saved_model","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submit to Competition\n1. \"Save & Run All\" (commit) this Notebook\n1. Go to the notebook viewer\n1. Go to \"Data\" section and find submit.tar.gz file.\n1. Click \"Submit to Competition\"\n1. Go to [My Submissions](https://www.kaggle.com/c/football/submissions) to view your score and episodes being played."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}